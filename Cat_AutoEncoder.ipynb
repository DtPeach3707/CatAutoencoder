{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Library imports"
      ],
      "metadata": {
        "id": "yZKtUPDkd8O3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxYcVyzGsr3L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import operator\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense, Input\n",
        "from keras.layers import Conv2D, Flatten\n",
        "from keras.layers import Reshape, Conv2DTranspose, Dropout\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import optimizers\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "Athv1rjTeAP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyPbBMixsxOr"
      },
      "outputs": [],
      "source": [
        "!gdown https://drive.google.com/uc?id=1hNxU1Cht9SHiXLrqTDughcfxyYYeyB8R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5kPgIpXvJv1"
      },
      "outputs": [],
      "source": [
        "!unzip /content/Cat_Gan_2.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image preprocessing"
      ],
      "metadata": {
        "id": "y0fo9ryMeBq4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgv44Z0qszkL"
      },
      "outputs": [],
      "source": [
        "size1 = 96\n",
        "size2 = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDmbn6Kzs18I"
      },
      "outputs": [],
      "source": [
        "os.mkdir(\"Training\") #making the new Training and testing folders\n",
        "dirs = [\"Domino\", \"Stormy\"] #Making the names for the subdirec\n",
        "pardirs = [\"/content/Training\"]\n",
        "for pardir in pardirs:\n",
        "  for dir in dirs: \n",
        "    path = os.path.join(pardir, dir)\n",
        "    os.mkdir(path)\n",
        "datadir1 = \"/content/Cat_Gan_2/Train/Domino\"\n",
        "datadir2 = \"/content/Cat_Gan_2/Train/Stormy\"\n",
        "datadir3 = \"/content/Cat_Gan_2/Test/Domino\"\n",
        "datadir4 = \"/content/Cat_Gan_2/Test/Stormy\"\n",
        "filelist1 = sorted(os.listdir(datadir1), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "filelist2 = sorted(os.listdir(datadir2), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "filelist3 = sorted(os.listdir(datadir3), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "filelist4 = sorted(os.listdir(datadir4), key = lambda fname: int(fname.split(\"_\")[0][-4:]))\n",
        "datadirs = [filelist1, filelist2, filelist3, filelist4]\n",
        "inc = 0\n",
        "idom = 0\n",
        "istorm = 0\n",
        "for filelist in datadirs:\n",
        "  for fil in filelist:\n",
        "    if inc == 0:\n",
        "      path = \"/content/Cat_Gan_2/Train/Domino/\" + fil\n",
        "      idom += 1\n",
        "      img = cv2.imread(path)\n",
        "      imgResized = cv2.resize(img, (size1, size2))\n",
        "      cv2.imwrite('/content/Training/Domino/DominoTR%03i.jpg' %idom, imgResized)\n",
        "    elif inc == 1:\n",
        "      path = \"/content/Cat_Gan_2/Train/Stormy/\" + fil\n",
        "      istorm += 1\n",
        "      img = cv2.imread(path)\n",
        "      imgResized = cv2.resize(img, (size1, size2))\n",
        "      cv2.imwrite('/content/Training/Stormy/StormyTR%03i.jpg' %istorm, imgResized)\n",
        "    elif inc == 2:\n",
        "      path = \"/content/Cat_Gan_2/Test/Domino/\" + fil\n",
        "      img = cv2.imread(path)\n",
        "      idom += 1\n",
        "      imgResized = cv2.resize(img, (size1, size2))\n",
        "      cv2.imwrite('/content/Training/Domino/DominoTR%03i.jpg' %idom, imgResized)\n",
        "    else:\n",
        "      path = \"/content/Cat_Gan_2/Test/Stormy/\" + fil\n",
        "      img = cv2.imread(path)\n",
        "      istorm += 1\n",
        "      imgResized = cv2.resize(img, (size1, size2))\n",
        "      cv2.imwrite('/content/Training/Stormy/StormyTR%03i.jpg' %istorm, imgResized)\n",
        "  inc += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw3FzBThs_Td"
      },
      "outputs": [],
      "source": [
        "datadirDom = '/content/Training/Domino'\n",
        "datadirStorm = '/content/Training/Stormy'\n",
        "datadirDom = os.listdir(datadirDom)\n",
        "datadirStorm = os.listdir(datadirStorm)\n",
        "labels = []\n",
        "cat_images = []\n",
        "for fil in datadirDom:\n",
        "  path = '/content/Training/Domino/' + fil\n",
        "  image = Image.open(path)\n",
        "  horz_image = np.array(image.transpose(method = Image.FLIP_LEFT_RIGHT))\n",
        "  vert_image = np.array(image.transpose(method = Image.FLIP_TOP_BOTTOM))\n",
        "  rot_image = np.array(image.rotate(180))\n",
        "  image = np.array(image)\n",
        "  cat_images.append(image)\n",
        "  plt.imshow(image)\n",
        "  cat_images.append(horz_image)\n",
        "  cat_images.append(vert_image)\n",
        "  cat_images.append(rot_image)\n",
        "for fil in datadirStorm:\n",
        "  path = '/content/Training/Stormy/' + fil\n",
        "  image = Image.open(path)\n",
        "  horz_image = np.array(image.transpose(method = Image.FLIP_LEFT_RIGHT))\n",
        "  vert_image = np.array(image.transpose(method = Image.FLIP_TOP_BOTTOM))\n",
        "  rot_image = np.array(image.rotate(180))\n",
        "  image = np.array(image)\n",
        "  cat_images.append(image)\n",
        "  cat_images.append(horz_image)\n",
        "  cat_images.append(vert_image)\n",
        "  cat_images.append(rot_image)\n",
        "cat_images = np.array(cat_images) / 255\n",
        "print (cat_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Cat Autoencoder Class"
      ],
      "metadata": {
        "id": "Vtfq33nbeFLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CatAutoencoder:\n",
        "  def __init__(self, latent_dim, img_size):\n",
        "    self.latent_dim = latent_dim\n",
        "    self.img_size = img_size\n",
        "    self.encoder = self.build_encoder()\n",
        "    self.decoder = self.build_decoder()\n",
        "\n",
        "    inputs = Input(shape=(self.img_size[0], self.img_size[1], 3))\n",
        "\n",
        "    output = self.decoder(self.encoder(inputs))\n",
        "    self.autoencoder = Model(inputs, output, name='Cat_Autoencoder')\n",
        "\n",
        "    self.autoencoder.compile(loss='mae', optimizer='adam')\n",
        "  \n",
        "  def build_encoder(self):\n",
        "\n",
        "    # Variables\n",
        "\n",
        "    layer_filters = [128, 96]\n",
        "    kernel_size = 6\n",
        "\n",
        "    # Layers\n",
        "\n",
        "    inputs = Input(shape=(self.img_size[0], self.img_size[1], 3))  # RGB Image\n",
        "    result = inputs\n",
        "    for filters in layer_filters:\n",
        "      result = Conv2D(filters, kernel_size, strides=2, padding='same', activation='relu')(result)\n",
        "    \n",
        "    result = Flatten()(result)\n",
        "    result = Dropout(0.4)(result)\n",
        "    result = Dense(100)(result)\n",
        "    result = Dropout(0.1)(result)\n",
        "    latent = Dense(self.latent_dim, activation='sigmoid', name='latent_vector')(result)\n",
        "\n",
        "    encoder = Model(inputs, latent, name='Cat_Encoder')\n",
        "    print(encoder.summary())\n",
        "    return encoder\n",
        "\n",
        "  def build_decoder(self):\n",
        "\n",
        "    # Variables\n",
        "\n",
        "    layer_filters = [96, 128]\n",
        "    kernel_size = 6\n",
        "\n",
        "    # Layers\n",
        "\n",
        "    inputs = Input(shape=self.latent_dim)\n",
        "    result = Dense(int((self.img_size[0] * self.img_size[1] * 128) / 2**(len(layer_filters)*2)))(inputs)\n",
        "    result = Reshape((int(self.img_size[0] / 2**len(layer_filters)), int(self.img_size[1] / 2**len(layer_filters)), 128))(result)\n",
        "    for filters in layer_filters:\n",
        "      result = Conv2DTranspose(filters, kernel_size, activation='relu', padding='same', strides=2)(result)\n",
        "    outputs = Conv2DTranspose(3, kernel_size, activation='sigmoid', padding='same')(result)\n",
        "\n",
        "    decoder = Model(inputs, outputs, name='Cat_Decoder')\n",
        "    print(decoder.summary())\n",
        "    return decoder\n",
        "  \n",
        "  def train(self, images, episodes=500, batch_size=50, gen_interval=100, save_model=True, plot_graphs=True, verbose=True):\n",
        "    tr_images, ts_images = train_test_split(images)\n",
        "    epochs = []\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    for i in range(int(episodes/gen_interval)):\n",
        "      history = self.autoencoder.fit(tr_images, tr_images, validation_data=(ts_images, ts_images), epochs=gen_interval, batch_size=batch_size, verbose=verbose)\n",
        "      for j in range(gen_interval):\n",
        "        epochs.append(gen_interval*i + j + 1)\n",
        "      for loss in history.history['loss']:\n",
        "        losses.append(loss)\n",
        "      for val_loss in history.history['val_loss']:\n",
        "        val_losses.append(val_loss)\n",
        "      decoded = self.autoencoder.predict(ts_images)\n",
        "      plt.imshow(ts_images[0])\n",
        "      plt.show()\n",
        "      plt.imshow(decoded[0])\n",
        "      plt.show()\n",
        "      imgs = np.concatenate([ts_images[:8], decoded[:8]])\n",
        "      imgs = imgs.reshape((4, 4, size2, size1, 3))\n",
        "      imgs = np.vstack([np.hstack(i) for i in imgs])\n",
        "      plt.figure()\n",
        "      plt.axis('off')\n",
        "      plt.title('%d: [Input: 1st 2 rows, Decoded: last 2 rows]' % (i+1))\n",
        "      plt.imshow(imgs, interpolation='none', cmap='gray')\n",
        "      plt.show()\n",
        "    \n",
        "    if plot_graphs:\n",
        "      plt.scatter(epochs, losses, label='training_loss')\n",
        "      plt.scatter(epochs, val_losses, label='testing_loss')\n",
        "      plt.legend(loc='upper right')\n",
        "      plt.xlabel('Epochs')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.savefig('autoencoder_loss_graph.png')\n",
        "      plt.show()\n",
        "    \n",
        "    if save_model:\n",
        "      self.encoder.save('Cat_Encoder.h5')\n",
        "      self.decoder.save('Cat_Decoder.h5')\n",
        "      "
      ],
      "metadata": {
        "id": "otZDZ1P1yakV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiating and Training Networks"
      ],
      "metadata": {
        "id": "idXK4EjotBNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = CatAutoencoder(100, (size2, size1))\n",
        "autoencoder.train(cat_images)"
      ],
      "metadata": {
        "id": "7IsrGH5xtn7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cat_AutoEncoder",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}